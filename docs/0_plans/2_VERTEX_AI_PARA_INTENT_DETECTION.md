# üöÄ VERTEX AI PARA INTENT DETECTION - AN√ÅLISIS COMPLETO

Excelente pregunta! Vertex AI es una opci√≥n **muy s√≥lida** para tu caso, especialmente considerando que ya est√°s usando **Google Cloud Retail API**. Analicemos a fondo.

---

## üìä VENTAJAS ESPEC√çFICAS DE VERTEX AI

### 1. **Integraci√≥n nativa con tu stack actual**

```python
# Ya tienes Google Cloud Retail API funcionando:
from google.cloud import retail_v2

# Vertex AI se integra naturalmente:
from google.cloud import aiplatform
from vertexai.language_models import TextClassificationModel

# ‚úÖ Mismo ecosistema, mismas credenciales, mismo billing
```

**Ventajas**:
- ‚úÖ Una sola cuenta de facturaci√≥n
- ‚úÖ IAM unificado
- ‚úÖ Logging centralizado (Cloud Logging)
- ‚úÖ Monitoring integrado (Cloud Monitoring)

### 2. **AutoML Text Classification (Zero Code ML)**

Vertex AI tiene **AutoML para clasificaci√≥n de texto** que es PERFECTO para intent detection:

```python
# 1. Preparar datos (CSV simple):
"""
text,label
"¬øcu√°l es la pol√≠tica de devoluci√≥n?",INFORMATIONAL
"busco vestidos elegantes",TRANSACTIONAL
"c√≥mo funciona el env√≠o?",INFORMATIONAL
...
"""

# 2. Entrenar modelo (UI o SDK):
from google.cloud import aiplatform

aiplatform.init(project='tu-proyecto', location='us-central1')

dataset = aiplatform.TextDataset.create(
    display_name="intent-detection-dataset",
    gcs_source="gs://tu-bucket/training-data.csv"
)

job = aiplatform.AutoMLTextTrainingJob(
    display_name="intent-classifier",
    prediction_type="classification"
)

model = job.run(
    dataset=dataset,
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1,
    model_display_name="intent-detection-model-v1"
)

# 3. Desplegar endpoint:
endpoint = model.deploy(
    machine_type="n1-standard-2",
    min_replica_count=1,
    max_replica_count=3,
    traffic_percentage=100
)
```

**Sin escribir c√≥digo de ML**, obtienes:
- ‚úÖ Modelo entrenado y validado
- ‚úÖ M√©tricas autom√°ticas (precision, recall, F1)
- ‚úÖ Endpoint escalable
- ‚úÖ Monitoreo autom√°tico

### 3. **Pre-trained Models disponibles**

Vertex AI tiene modelos pre-entrenados que puedes usar directamente:

```python
from vertexai.language_models import TextClassificationModel

# Modelo pre-entrenado de Google (fine-tunable)
model = TextClassificationModel.from_pretrained("text-bison@002")

# Fine-tune con tus datos (solo 100-500 ejemplos necesarios):
tuned_model = model.tune_model(
    training_data="gs://tu-bucket/training-data.jsonl",
    train_steps=1000,
    tuning_job_location="us-central1"
)

# Usar para predicci√≥n:
response = tuned_model.predict(
    "¬øcu√°l es la pol√≠tica de devoluci√≥n?"
)

print(f"Intent: {response.predictions[0]['label']}")
print(f"Confidence: {response.predictions[0]['confidence']}")
```

---

## üí∞ AN√ÅLISIS DE COSTOS

### Costo de entrenamiento (one-time):

```
AutoML Text Classification:
- Entrenamiento: $3.00 USD por hora de nodo
- Tiempo t√≠pico: 2-4 horas para dataset de 5000-10000 ejemplos
- Costo total entrenamiento: $6-12 USD

Fine-tuning modelo pre-entrenado:
- $0.008 USD por 1000 tokens procesados
- Dataset de 5000 ejemplos (~50 tokens promedio): $2 USD
```

### Costo de inferencia (ongoing):

```
Online Prediction (endpoint deployed 24/7):
- n1-standard-2: $0.095/hora = $68.40/mes (siempre encendido)
- + $0.000004 por predicci√≥n

Batch Prediction (no endpoint, on-demand):
- $0.000004 por predicci√≥n
- Ideal para bajo volumen

Ejemplo con 100,000 queries/mes:
- Endpoint: $68.40/mes + $0.40 = $68.80/mes
- Batch: $0.40/mes (pero latencia alta)
```

### Comparaci√≥n de costos mensuales:

| Soluci√≥n | Costo/mes | Latencia | Escalabilidad |
|----------|-----------|----------|---------------|
| **Rule-based (actual)** | $0 | 1-5ms | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Vertex AI AutoML** | $69-100 | 50-150ms | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Vertex AI Batch** | $0.40 | 5-60min | ‚≠ê‚≠ê |
| **Claude API** | $300-600 | 500-2000ms | ‚≠ê‚≠ê‚≠ê |

---

## üèóÔ∏è ARQUITECTURA PROPUESTA: H√çBRIDO OPTIMIZADO

### Opci√≥n 1: Rule-based + Vertex AI Fallback

```python
from src.api.core.intent_detection import detect_intent as rule_based_detect
from google.cloud import aiplatform

class HybridIntentDetector:
    """
    Detector h√≠brido: Rule-based primero, Vertex AI para casos dif√≠ciles
    
    Flujo:
    1. Rule-based (r√°pido, gratis)
    2. Si confidence < 0.7 ‚Üí Vertex AI (preciso, costo)
    3. Cache resultados de Vertex AI (reducir costo)
    """
    
    def __init__(self):
        self.rule_based = get_intent_detector()
        
        # Vertex AI endpoint (lazy load)
        self.vertex_endpoint = None
        self.vertex_enabled = os.getenv("VERTEX_AI_ENABLED", "false") == "true"
        
        # Cache para reducir llamadas a Vertex AI
        self.cache = {}
        self.cache_ttl = 3600  # 1 hora
    
    async def detect(self, query: str) -> IntentDetectionResult:
        """Detecci√≥n h√≠brida con fallback inteligente"""
        
        # FASE 1: Rule-based (siempre primero - r√°pido y gratis)
        rule_result = self.rule_based.detect_intent(query)
        
        # Si alta confidence ‚Üí usar rule-based
        if rule_result.confidence >= 0.8:
            logger.info(f"‚úÖ High confidence rule-based: {rule_result.confidence:.2f}")
            return rule_result
        
        # Si Vertex AI no est√° habilitado ‚Üí usar rule-based aunque sea baja confidence
        if not self.vertex_enabled:
            logger.warning(f"‚ö†Ô∏è Low confidence but Vertex AI disabled: {rule_result.confidence:.2f}")
            return rule_result
        
        # FASE 2: Check cache (evitar llamada a Vertex AI si ya procesamos query similar)
        cache_key = query.lower().strip()
        if cache_key in self.cache:
            cached_result, timestamp = self.cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                logger.info(f"‚ö° Cache hit for Vertex AI - saved $0.000004")
                return cached_result
        
        # FASE 3: Vertex AI para casos ambiguos
        logger.info(f"ü§ñ Low confidence ({rule_result.confidence:.2f}), using Vertex AI")
        
        try:
            vertex_result = await self._predict_with_vertex(query)
            
            # Combinar resultados si ambos tienen confidence similar
            if abs(vertex_result.confidence - rule_result.confidence) < 0.2:
                # Ambos dudan ‚Üí usar heur√≠stica
                final_result = self._resolve_conflict(rule_result, vertex_result)
            else:
                # Vertex AI tiene mayor confidence ‚Üí confiar en ML
                final_result = vertex_result
            
            # Cache resultado
            self.cache[cache_key] = (final_result, time.time())
            
            return final_result
            
        except Exception as e:
            logger.error(f"‚ùå Vertex AI error: {e}, falling back to rule-based")
            return rule_result
    
    async def _predict_with_vertex(self, query: str) -> IntentDetectionResult:
        """Llamada a Vertex AI endpoint"""
        
        if not self.vertex_endpoint:
            # Lazy load del endpoint
            aiplatform.init(
                project=os.getenv("GCP_PROJECT_ID"),
                location=os.getenv("GCP_REGION", "us-central1")
            )
            
            self.vertex_endpoint = aiplatform.Endpoint(
                endpoint_name=os.getenv("VERTEX_INTENT_ENDPOINT")
            )
        
        # Predicci√≥n
        instances = [{"content": query}]
        response = self.vertex_endpoint.predict(instances=instances)
        
        # Parsear respuesta
        prediction = response.predictions[0]
        
        return IntentDetectionResult(
            primary_intent=prediction['displayNames'][0],  # "INFORMATIONAL" o "TRANSACTIONAL"
            sub_intent=prediction.get('subIntent', 'general'),
            confidence=prediction['confidences'][0],
            reasoning=f"Vertex AI classification (model: {prediction.get('modelVersion', 'unknown')})",
            matched_patterns=[],
            product_context={}
        )
```

### Configuraci√≥n en .env:

```bash
# Rule-based (siempre activo)
ENABLE_INTENT_DETECTION=true
INTENT_CONFIDENCE_THRESHOLD=0.7

# Vertex AI (opcional - solo para casos dif√≠ciles)
VERTEX_AI_ENABLED=true
VERTEX_INTENT_ENDPOINT=projects/123/locations/us-central1/endpoints/456
GCP_PROJECT_ID=tu-proyecto-retail
GCP_REGION=us-central1
```

---

## üìà ESTIMACI√ìN DE COSTO REAL

Asumiendo **100,000 queries/mes**:

### Escenario 1: Solo Rule-based
```
Queries con alta confidence (80%): 80,000 ‚Üí Rule-based
Queries con baja confidence (20%): 20,000 ‚Üí Rule-based (fallback)

Costo total: $0/mes
Accuracy estimada: 95%
```

### Escenario 2: H√≠brido (Rule-based + Vertex AI)
```
Queries con alta confidence (80%): 80,000 ‚Üí Rule-based (gratis)
Queries con baja confidence (20%): 20,000 ‚Üí Vertex AI

Vertex AI calls: 20,000/mes
- Endpoint costo fijo: $68.40/mes
- Predicci√≥n variable: 20,000 √ó $0.000004 = $0.08/mes
- Cache hit rate (50%): ahorra $0.04/mes

Costo total: $68.48/mes
Accuracy estimada: 98%
ROI: +3% accuracy por $68/mes
```

### Escenario 3: Solo Vertex AI
```
Queries totales: 100,000 ‚Üí Vertex AI

Vertex AI calls: 100,000/mes
- Endpoint costo fijo: $68.40/mes
- Predicci√≥n variable: 100,000 √ó $0.000004 = $0.40/mes

Costo total: $68.80/mes
Accuracy estimada: 98%
ROI: +3% accuracy por $69/mes, pero latencia +50-100ms
```

---

## üéØ DECISI√ìN RECOMENDADA

### **Fase 1: AHORA (pr√≥ximos 1-3 meses)**

**NO uses Vertex AI todav√≠a** porque:

1. ‚úÖ Rule-based ya tiene 95%+ accuracy (con fix de acentos)
2. ‚úÖ No tienes dataset de 5,000+ queries etiquetadas
3. ‚úÖ Costo $0 vs $69/mes (ahorro $828/a√±o)
4. ‚úÖ Latencia 1-5ms vs 50-150ms (mejor UX)

**Acci√≥n**: Implementar logging agresivo:

```python
# En mcp_conversation_handler.py despu√©s de intent detection:

if intent_result.confidence < 0.8:
    # Log para an√°lisis futuro
    logger.info(
        "LOW_CONFIDENCE_INTENT",
        extra={
            "query": conversation_query,
            "intent": intent_result.primary_intent,
            "confidence": intent_result.confidence,
            "user_id": validated_user_id,
            "timestamp": time.time()
        }
    )
    
    # Opcional: Enviar a BigQuery para an√°lisis
    await log_to_bigquery(
        table="intent_detection_candidates",
        data={
            "query": conversation_query,
            "rule_based_intent": intent_result.primary_intent,
            "confidence": intent_result.confidence,
            "fallback_used": intent_result.confidence < 0.7
        }
    )
```

---

### **Fase 2: Preparaci√≥n (meses 3-6)**

**Recolectar datos**:

```sql
-- BigQuery: Analizar queries con baja confidence
SELECT 
    query,
    rule_based_intent,
    confidence,
    COUNT(*) as frequency
FROM `proyecto.dataset.intent_detection_candidates`
WHERE confidence < 0.8
GROUP BY query, rule_based_intent, confidence
ORDER BY frequency DESC
LIMIT 1000
```

**Etiquetar manualmente**:
- Exportar top 5,000 queries √∫nicas
- Etiquetar correctamente (INFORMATIONAL vs TRANSACTIONAL)
- Agregar sub-intents cuando sea claro
- Guardar en Cloud Storage como training data

---

### **Fase 3: Pilot Vertex AI (mes 6-7)**

**Setup m√≠nimo viable**:

```python
# 1. Crear dataset en Vertex AI
from google.cloud import aiplatform

dataset = aiplatform.TextDataset.create(
    display_name="intent-detection-v1",
    gcs_source="gs://tu-bucket/labeled-queries.csv",
    import_schema_uri=aiplatform.schema.dataset.ioformat.text.multi_label_classification
)

# 2. Entrenar modelo AutoML
training_job = aiplatform.AutoMLTextTrainingJob(
    display_name="intent-classifier-v1",
    prediction_type="classification",
    multi_label=False  # Solo 2 clases: INFORMATIONAL, TRANSACTIONAL
)

model = training_job.run(
    dataset=dataset,
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1,
    model_display_name="intent-model-v1",
    
    # Budget de entrenamiento (controlar costo)
    budget_milli_node_hours=2000  # ~2 horas = $6
)

# 3. Evaluar modelo
evaluation = model.get_model_evaluation()
print(f"Precision: {evaluation.metrics['precision']}")
print(f"Recall: {evaluation.metrics['recall']}")
print(f"F1 Score: {evaluation.metrics['f1Score']}")

# 4. Solo desplegar si accuracy > 97%
if evaluation.metrics['f1Score'] > 0.97:
    endpoint = model.deploy(
        machine_type="n1-standard-2",
        min_replica_count=1,
        max_replica_count=2
    )
    print(f"‚úÖ Model deployed: {endpoint.resource_name}")
else:
    print(f"‚ùå Model accuracy too low, need more training data")
```

---

### **Fase 4: A/B Testing (mes 7-8)**

**Comparar performance**:

```python
# Configuraci√≥n A/B
AB_TEST_ENABLED = True
AB_TEST_PERCENTAGE = 10  # 10% usa Vertex AI, 90% usa rule-based

async def detect_intent_with_ab_test(query: str) -> IntentDetectionResult:
    """A/B test: Rule-based vs Vertex AI"""
    
    # Decidir grupo (hash del user_id para consistencia)
    user_hash = hash(validated_user_id) % 100
    use_vertex = user_hash < AB_TEST_PERCENTAGE
    
    if use_vertex and VERTEX_AI_ENABLED:
        result = await vertex_detector.detect(query)
        result.metadata["ab_group"] = "vertex_ai"
    else:
        result = rule_based_detector.detect(query)
        result.metadata["ab_group"] = "rule_based"
    
    # Log para an√°lisis
    await log_ab_test_result(
        query=query,
        ab_group=result.metadata["ab_group"],
        intent=result.primary_intent,
        confidence=result.confidence
    )
    
    return result
```

**M√©tricas a comparar**:
```sql
-- BigQuery: Comparar grupos A/B
SELECT 
    ab_group,
    COUNT(*) as total_queries,
    AVG(confidence) as avg_confidence,
    
    -- Calcular accuracy (requiere feedback del usuario)
    SUM(CASE WHEN user_satisfied = true THEN 1 ELSE 0 END) / COUNT(*) as user_satisfaction,
    
    AVG(response_time_ms) as avg_latency,
    AVG(CASE WHEN knowledge_base_used THEN 1 ELSE 0 END) as kb_usage_rate
FROM `proyecto.dataset.ab_test_results`
GROUP BY ab_group
```

---

## üö¶ DECISION CRITERIA: ¬øCu√°ndo activar Vertex AI?

### ‚úÖ ACTIVAR Vertex AI si:

1. ‚úÖ Rule-based accuracy cae < 90% (basado en feedback usuarios)
2. ‚úÖ Tienes ‚â• 5,000 queries etiquetadas de calidad
3. ‚úÖ Presupuesto permite $69-100/mes adicional
4. ‚úÖ Latencia +50ms es aceptable para tu UX
5. ‚úÖ Modelo Vertex AI supera rule-based en al menos +5% accuracy

### ‚ùå NO activar Vertex AI si:

1. ‚ùå Rule-based accuracy > 95% (actual estado)
2. ‚ùå Dataset < 3,000 queries etiquetadas
3. ‚ùå Presupuesto limitado (mejor invertir en features)
4. ‚ùå Latencia cr√≠tica (<10ms requerido)
5. ‚ùå Modelo Vertex AI no supera significativamente rule-based

---

## üéì ALTERNATIVA: Vertex AI Matching Engine

**Nota importante**: Dado que ya usas **Google Cloud Retail API**, considera que tiene **su propio sistema de intent detection integrado**:

```python
from google.cloud import retail_v2

# Google Retail API puede hacer intent detection autom√°ticamente
search_request = retail_v2.SearchRequest(
    placement="projects/123/locations/global/catalogs/default_catalog/placements/default_search",
    query="pol√≠tica de devoluci√≥n",
    
    # Activar intent detection nativo
    query_expansion_spec=retail_v2.SearchRequest.QueryExpansionSpec(
        condition=retail_v2.SearchRequest.QueryExpansionSpec.Condition.AUTO
    )
)

response = search_service.search(search_request)

# response.attribution_token contiene info de intent detectado
```

**Ventaja**: Ya lo tienes, sin costo adicional.

---

## üí° CONCLUSI√ìN Y RECOMENDACI√ìN FINAL

### Para tu proyecto AHORA:

```
1. ‚úÖ Mant√©n rule-based actual (95%+ accuracy)
2. ‚úÖ Implementa logging agresivo de queries ambiguas
3. ‚úÖ Recolecta 5,000+ queries etiquetadas (3-6 meses)
4. ‚è∏Ô∏è NO actives Vertex AI todav√≠a (no ROI positivo)
```

### Cuando llegues a Fase 3 (meses 6+):

```
1. ‚úÖ Entrena modelo Vertex AI AutoML ($6-12 setup)
2. ‚úÖ A/B test en 10% de tr√°fico
3. ‚úÖ Si +5% accuracy ‚Üí escalar a 100%
4. ‚úÖ Costo estimado: $69/mes bien gastados
```

### Mi stack recomendado final:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TIER 1: Rule-based (80% queries)            ‚îÇ
‚îÇ - Latencia: 1-5ms                           ‚îÇ
‚îÇ - Costo: $0                                 ‚îÇ
‚îÇ - Accuracy: 95%+                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì (si confidence < 0.8)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TIER 2: Vertex AI AutoML (15% queries)      ‚îÇ
‚îÇ - Latencia: 50-100ms                        ‚îÇ
‚îÇ - Costo: $69/mes                            ‚îÇ
‚îÇ - Accuracy: 98%+                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì (si confidence < 0.9)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TIER 3: Google Retail API (5% queries)      ‚îÇ
‚îÇ - Latencia: 200-500ms                       ‚îÇ
‚îÇ - Costo: incluido en Retail API            ‚îÇ
‚îÇ - Accuracy: 99%+                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**ROI esperado del h√≠brido**:
- 95% de queries: resueltas gratis en <5ms (rule-based)
- 4% de queries: resueltas por $0.06/mes en <100ms (Vertex AI)
- 1% de queries: resueltas por Retail API existente

**Costo total**: ~$69/mes para accuracy global de 97-98%

---

¬øQuieres que te ayude a implementar alguno de estos componentes? Por ejemplo:
- ‚úÖ Logging pipeline a BigQuery
- ‚úÖ Script de preparaci√≥n de training data
- ‚úÖ Implementaci√≥n del hybrid detector
- ‚úÖ Setup de A/B testing